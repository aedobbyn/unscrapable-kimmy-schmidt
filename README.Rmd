---
title: "README.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Who am I

# What is web scraping?

# Tooling

### rvest

### Selenium

### SelectorGadget

# Politeness

# Versions

In a terminal:
```
brew cask install chromedriver
chromedriver --version
```

```{r}
wdman:::chrome_check(verbose = TRUE)
```

```{r}
chromedriver_versions <- binman::list_versions(appname = "chromedriver") %>% 
  .[[1]]
```

### Check which version of Chrome you have

`system` is a way to use the terminal through R. (You can replace this with whatever the path to Chrome is on your computer.)

```{r}
cmd <- "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --version"
```


```{r}
chrome_version <- system(cmd, intern = TRUE) %>% 
  str_extract("[0-9\\.]+") %>% 
  str_sub(1L, 9L)
```

# Use the chromedriver version that matches your Chrome version
```{r}
version <- chromedriver_versions[which(
  str_detect(chromedriver_versions, chrome_version)
  )]
```



# Example

A random Wikipedia page

```{r}
next_xpath <- "/html/body/div/div/div[2]/button[1]"
```

```{r}
start_session <- function(url, browser = "chrome", port = 4444L, version) {
  if (port == 4444L) {
    while (any(!is.na(pingr::ping_port("localhost", port)))) {
      port <- port + 1
    }
  }
  
  # Start chrome driver
  wdman::chrome(
        port = as.integer(port),
        version = version,
        check = FALSE
      )
  
  # Create the driver object on localhost
  seleniumPipes::remoteDr(browserName = "chrome", port = port, version = version) %>%
    # Go to the url
    seleniumPipes::go(url)
}
```


```{r}
url <- "https://en.wikipedia.org/wiki/Special:Random"

sess <- start_session(url, version = version)
```

```{r}
click <- function(sess, id_type, unique_id) {
  seleniumPipes::findElement(sess, id_type, unique_id) %>%
    seleniumPipes::elementClick()
}
```

```{r}
sess %>% 
  seleniumPipes::go(url)
```

```{r}
extract_html <- function(sess) {
  sess %>%
    seleniumPipes::getPageSource() %>%
    as.character() %>%
    xml2::read_html()
}
```

What if now we wanted all the links on this page? `rvest::html_text` would give us the text of the links. `rvest::html_attr` will give us attributes about the thing we're scraping.

```{r}
links <- sess %>% 
  extract_html() %>% 
  rvest::html_nodes("a") %>% 
  rvest::html_attr("href")
```

```{r}
text <- 
  sess %>% 
  extract_html() %>% 
  rvest::html_nodes("a") %>% 
  rvest::html_text()
```

```{r}
link_tbl <- 
  tibble(
    text = text,
    link = links
  ) %>% 
  print(n = nrow(.))
```

```{r}
random_link_tbl <- 
  link_tbl %>% 
  filter(
    str_detect(
      link, "^/wiki"
    ) &
      ! str_detect(link, ":")
  ) %>% 
  sample_n(1)

new_link <- glue::glue("https://en.wikipedia.org{random_link_tbl$link}")
```

```{r}
message(glue::glue("Going to {new_link}!"))

sess %>% 
  seleniumPipes::go(new_link)
```


# seleniumPipes has lots of useful functions like `back`, `refresh`, `getCurrentUrl`

# We can even take a screenshot

```{r}
sess %>% 
  seleniumPipes::takeScreenshot(
    file = glue::glue("{here::here()}/screenshot.png")
  )
```


What if we wanted to enter text?

```{r}
input_text <- function(sess, id_type, unique_id, text, clear = TRUE) {
  element <- seleniumPipes::findElement(sess, id_type, unique_id)
  
  if (clear) seleniumPipes::elementClear(element)
  
  seleniumPipes::elementSendKeys(element, text)
}
```

We first need to find the element on the page. Then we can enter text with `seleniumPipes::elementSendKeys`.

```{r}
text_search_name <- "search"

sess %>% 
  input_text(
    id_type = "name",
    unique_id = text_search_name,
    text = "Amanda Bynes"
  )

element <- 
  seleniumPipes::findElement(sess, id_type = "name", unique_id = text_search_name)

element %>% seleniumPipes::elementSendKeys(key = "enter")
```





```{r}
library(tidyverse)
library(rvest)
library(foodpls)

url <- "http://wikiroulette.co/"

driver <- seleniumPipes::remoteDr(browserName = "chrome")

sess <- start_session(url, version = version)

(xml <- 
  url %>% 
  xml2::read_html()
)
```

xml by itself is not very useful to us

```{r}
str(xml)
```

This is where `rvest` comes in. 

There are two types of nodes you can supply to `rvest::html_nodes`: CSS selectors and xpaths.

```{r}
?rvest::html_nodes
```


```{r}
(
  xml %>% 
    rvest::html_nodes()
)
```




```{r}
url <- "https://fivethirtyeight.com/"
```


```{r}
(xml <- 
  url %>% 
  xml2::read_html()
)
```

```{r}
(nodes <- xml %>% 
  rvest::html_nodes("a")
)
```

```{r}
(text <- nodes %>% 
  rvest::html_text())
```
```{r}
clean_html <- function(x) {
  x %>%
    str_squish() %>%
    str_remove_all("[\\n\\r\\t]")
}
```

```{r}
text[1:10]

text[1:10] %>% 
  clean_html()
```




